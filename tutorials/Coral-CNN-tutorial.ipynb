{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Convolutional Neural Network\n",
    "\n",
    "Coral reefs are among the most diverse and valuable ecosystems on Earth, providing habitat for 25% of all marine species and supporting the livelihoods of over half a billion people worldwide. However, these ecosystems face unprecedented threats from climate change, ocean acidification, and other human activities, with many species now endangered.\n",
    "\n",
    "We are given a dataset containing images of three different coral species:\n",
    " - *Acropora cervicornis* (Staghorn Coral)\n",
    " - *Colpophyllia natans* (Boulder Brain Coral)\n",
    " - *Montastraea cavernosa* (Greater Star Coral)\n",
    "\n",
    "Our task is to build a Convolutional Neural Network (CNN) that can classify the coral images into the correct species. This technology can help automate coral reef monitoring efforts and support conservation initiatives by enabling rapid, large-scale species identification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building a CNN Model from Scratch\n",
    "### Step 0. Check GPU Availability and TensorFlow Version\n",
    "\n",
    "Before training deep learning models, it's important to check whether TensorFlow can access a GPU. Training on a GPU is significantly faster than on a CPU, especially for large image image datasets.\n",
    "\n",
    "If you've followed the setup instructions in the [GitHub README](https://github.com/kbeavers/coral-species-CNN-tutorial/blob/main/README.md), and you've run the `install_kernel.sh` script on **Frontera**, you should now be running this notebook inside a containerized Jupyter kernel that includes:\n",
    "- TensorFlow with GPU support\n",
    "- CUDA libraries compatible with the system\n",
    "- All required Python packages pre-installed\n",
    "\n",
    "This cell will confirm that your environment is correctly configured (TIP: Make sure you change your kernel to `tf-cuda101`.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Data Loading and Organization\n",
    "In this step, we load all coral images from the dataset directory and organize them into a DataFrame. Each image is assigned a label based on the name of the directory it's stored in (i.e., 'ACER' - *Acropora cervicornis*, 'CNAT' - *Colpophyllia natans*, 'MCAV' - *Montastraea cavernosa*).\n",
    "\n",
    "This DataFrame will serve as the foundation for splitting our data into training, validation, and test sets later in the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. List Dataset Directory Contents\n",
    "Before loading the images, we first want to inspect the directory structure to make sure everything is in the right place. The code below lists the contents of the `coral-species` data directory to verity that the subdirectories for each coral species are present and correctly named:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Check File Extensions\n",
    "Next, we scan the dataset directory and all its subdirectories to find out what types of image files are present. This helps us catch unexpected or unsupported file types (e.g., GIFs, txt files, etc.), which could cause problems later when loading images.\n",
    "\n",
    "This also allows us to see if the images are all in the same format or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Explore Image Dimensions and Color Modes\n",
    "Before feeding images into a CNN, it's important to understand the basic properties of the dataset. In this step, we examine the **dimensions** (width x height) as well as the **color mode** (e.g., RGB, RGBA, grayscale) of each image. This helps us decide if we need to resize or convert images before we begin training our CNN.\n",
    "\n",
    "The script below prints a summary and gives recommendations if inconsistencies are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset analysis reveals some important characteristics that we'll need to keep in mind as we proceed with the tutorial:\n",
    " 1. **Image Size Variation**: We have 451 total images in our dataset, with 88 different image sizes (dimensions). Also notice that some images are in portrait orientation (height > width) while others are landscape (width > height). CNNs expect all images to have the same dimensions, so we'll need to resize them to a standard size before training our model. \n",
    " 2. **Color Mode**: All images share the same color mode. Great!\n",
    "\n",
    "We will address this again in Step 4 when we prepare our data for input into the CNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Check for Corrupted Images\n",
    "\n",
    "Before continuing, we want to make sure that all image files are readable. Corrupted files can break your model training or cause unexpected errors during preprocessing.\n",
    "\n",
    "In this step, we:\n",
    "1. Attempt to open each '.jpg' file using PIL\n",
    "2. Discard any files that fail to load\n",
    "\n",
    "This ensures we only keep clean, valid images for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any corrupted images in your dataset, this code will automatically remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. Create a DataFrame of Image Paths and Labels\n",
    "Now that we have taken a peak at the format of our data and have removed any corrupted images, we can start setting up our data for training. In this step, we build a `pandas.DataFrame` that organizes all the image data into two columns:\n",
    " 1. **filepath**: The full path to each image file\n",
    " 2. **label**: The class label for each image, taken from the directory name\n",
    "\n",
    "This structured DataFrame is essential for training with Keras' `flow_from_dataframe` method that we'll use later in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Visualize the Data\n",
    "### 2.1 Visualize the Class Distribution\n",
    "Before we start training, it's important to understand how many images we have for each class (in this case, coral species).\n",
    "\n",
    "In this step we:\n",
    " 1. Count how many images belong to each class\n",
    " 2. Plot the class distribution as a pie chart and bar graph\n",
    "\n",
    "If the dataset is imbalanced (i.e., some classes have far more images than others), we may need to account for this later using **class weights** or **data augmentation**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Challenge**: Describe the class distribution in your own words. How much of the dataset is made up by the largest class? The smallest class? Is there anything that we need to address before continuing?\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "Our dataset contains three coral species: ACER (*Acropora cervicornis*), MCAV (*Montastraea cavernosa*), and CNAT (*Colpophyllia natans*). The class distribution is well balanced, with the largest class (ACER) making up ~35% of the dataset and the smallest class (CNAT) making up ~31% of the dataset. \n",
    "\n",
    "Because all three classes are similarly represented, we do not need to apply class weighting or balancing techniques during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Visualize Images from the Dataset\n",
    "It's helpful to look at a few images from each class to get a better understanding of the dataset. This will give us a better sense of:\n",
    " - What each coral species looks like\n",
    " - How much visual variation exists within each class (e.g., different angles, lighting, etc.)\n",
    " - Whether the dataset includes noise, blur, or other artifacts\n",
    "\n",
    "We'll display a grid of randomly selected images, grouped by class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Challenge**: Try changing the `random.seed` value a few times to view different images from our dataset. What do you notice? Take a moment to write down your observations.\n",
    "\n",
    "*Remember: the quality of a machine learning model is decided largely by the quality of the dataset it was trained on!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split the Dataset and Handle Class Imbalance\n",
    "\n",
    "#### 3.1. Split the Dataset into Training, Validation and Test Sets\n",
    "We are now ready to split our labelled image dataset into three parts:\n",
    "\n",
    " - **Training Set**: Used to train the model\n",
    " - **Validation Set**: Used to tune hyperparameters and monitor training\n",
    " - **Test Set**: Used to evaluate the final model's performance after training is complete\n",
    "\n",
    "We use `train_test_split()` from scikit-learn in two stages:\n",
    "\n",
    " 1. First, we split the original dataset into **training + test** sets\n",
    " 2. Then, we split the training set again into **training + validation**.\n",
    "\n",
    "This ensures that our CNN *never sees the test set* during training, which is important for obtaining an unbiased estimate of the model's performance. \n",
    "\n",
    "To preserve the class distribution across all splits, we use `stratify=df[\"label\"]` to ensure each subset reflects the original proportions of each class. This is called **stratified sampling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: Compute Class Weights\n",
    "\n",
    "If our dataset is imbalanced (i.e., some classes have many more images than others), the model may learn to favor those majority classes. To address this, we can compute **class weights** based on the training data using the `compute_class_weight` function from scikit-learn.\n",
    "\n",
    "These weights:\n",
    " - Assign higher important to underrepresented classes\n",
    " - Are passed into `model.fit()` using the `class_weight` argument\n",
    " - Adjust how the loss is calculated during training\n",
    "\n",
    "This technique helps the model give balanced attention to all classes during training.\n",
    "\n",
    "While our dataset is quite balanced, we provide the code for computing class weights below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Image Preprocessing and Data Generators\n",
    "As we discovered in Step 1.3, we need to prepare our images before feeding them into the CNN. This step involves two key concepts:\n",
    "\n",
    "**a. Data Generators**\n",
    "\n",
    "Data generators are special tools that help us efficiently load and preprocess image data in small batches (instead of all at once). Keras provides a built-in data generator called `ImageDataGenerator` that can:\n",
    " - Resize all images to a consistent size\n",
    " - Normalize pixel values (e.g., from [0-255] to [0-1])\n",
    " - Augment the training data with random transformations to improve generalization\n",
    "\n",
    "Data generators can be used with Keras model methods like `fit()`, `evaluate()`, and `predict()`, which is particularly useful when dealing with large datasets that don't all fit into memory at once. \n",
    "\n",
    "**b. Data Augmentation**\n",
    "\n",
    "Data augmentation is a powerful technique that helps our model learn more robust features by creating variations of our training images. Augmentation techniques not only expand the size of our training set, but also help prevent overfitting by exposing our model to different variations of our images. \n",
    "\n",
    "Conveniently, `ImageDataGenerator` also provides a number of built-in augmentation techniques that we can use to augment our training data, such as:\n",
    " - Random rotations\n",
    " - Zooming in or out\n",
    " - Shifting the image left or right\n",
    " - Flipping the image horizontally\n",
    "\n",
    "Each of these modifications creates a new, slightly different version of our training images, helping our model learn to recognize the same features in different orientations.\n",
    "\n",
    "#### 4.1 Define Image Preprocessing and Augmentation\n",
    "We will define three separate `ImageDataGenerator` objects, one for each dataset split (train, val, test):\n",
    " - `train_datagen` will apply both normalization and augmentation to the training data\n",
    " - `val_datagen` and `test_datagen` will only apply normalization (no augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2 Load Images Using `flow_from_dataframe()`\n",
    "Now that our preprocessing methods are defined, we can use `flow_from_dataframe()` to load images in batches directly from our labeled DataFrames (`train_df`, `val_df`, and `test_df`).\n",
    "\n",
    "All generators return batches of preprocessed image tensors and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check: Inspect a Batch from the Training Generator\n",
    "\n",
    "Let's inspect the output of the `train_generator` to make sure it's working as expected.\n",
    "\n",
    "In the code below, we:\n",
    " - Retrieve one batch of images and labels from the training generator\n",
    " - Check the shape of the batch\n",
    " - Display a few image-label pairs to confirm the generator is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize a Few Images from the Training Generator\n",
    "\n",
    "Let's display a few images from the training generator along with their decoded class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Challenge**: Look carefully at the images displayed above. Try running the code cell multiple times and changing the code to display images from the validation and test generators. What do you notice about the images that you didn't see before (in Step 3)? Do you notice any differences in the images each time you run the cell? Think about why this might be happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Define Your CNN Architecture\n",
    "\n",
    "Congratulations! Our data is now ready to be used to train a Convolutional Neural Network (CNN) to classify our coral images. \n",
    "\n",
    "In this step, we will define the architecture of our CNN model. Below, we define a model that consists of three main parts:\n",
    "\n",
    "1. **Convolutional Blocks** (Feature Extraction):\n",
    "   - Block 1: 32 filters (3×3), followed by AveragePooling\n",
    "   - Block 2: 64 filters (3×3), followed by AveragePooling\n",
    "   - Block 3: 128 filters (3×3), followed by AveragePooling\n",
    "\n",
    "   Each block increases the number of filters, allowing the network to detect more complex patterns.\n",
    "\n",
    "2. **Flatten Layer**:\n",
    "   - Converts the 3D feature maps to a 1D vector for the dense layers\n",
    "\n",
    "3. **Dense Layers** (Classification):\n",
    "   - First dense layer: 128 perceptrons\n",
    "   - Second dense layer: 64 perceptrons\n",
    "   - Output layer: 3 perceptrons (one for each coral species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have filled in the blanks and defined your model, let's compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we use the `RMSprop` optimizer, which adapts the learning rate based on recent gradients, and is a popular chocie for image classification tasks. We also set the learning rate to `1e-4`, which sets the inital learning rate for the optimizer. \n",
    "\n",
    "*Note: while these are good starting choices, you may want to experiment with different optimizers or learning rates based on your model's performance*\n",
    "\n",
    "Finally, let's display our model architecture and parameter count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train the CNN Model\n",
    "\n",
    "Now that our CNN architecture is defined, we can train it using the `fit()` method.\n",
    "\n",
    "During training, the model will learn patterns in the training data and adjust is parameters to minimize the loss function. After each epoch, the model's performance is evaluated on the validation set.\n",
    "\n",
    "Here, we also pass in `class_weight` to demonstrate how to handle imbalanced data. \n",
    "\n",
    "We also track the training history, which we’ll use later to visualize performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=15,\n",
    "    class_weight=class_weight_dict # Computed in Step 4.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Training History\n",
    "\n",
    "After training the model, we can visualize the accuracy and loss over time to better understand how the model is learning. These plots can help us identify overfitting, underfitting, or confirm that the model is learning as expected.\n",
    "\n",
    "We can use the `cnn_history` object returned by the `fit()` method to plot the training and validation accuracy and loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above show the training and validation accuracy/loss over 15 epochs. \n",
    "\n",
    "**Thought Challenge**: What do you notice about the training and validation accuracy and loss? What does this tell you about the model's learning performance (i.e. overfitting, underfitting, or healthy learning)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Evaluate the Model on Test Set\n",
    "\n",
    "Now that we've trained our model, it's time to evaluate its performance on the test set. This step is crucial because it helps us understand how well the model generalizes to new, unseen data, which is a good indicator of its real-world performance.\n",
    "\n",
    "#### Evaluate Test Accuracy and Loss\n",
    "\n",
    "We use `model.evaluate()` to calculate the test accuracy and loss. These metrics give us a quick overview of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model correctly classifies the test images about 45% of the time, and our loss is still quite high. While these numbers provide a snapshot of performance, they don't tell the whole story. Let's dig deeper with a confusion matrix.\n",
    "\n",
    "#### Visualize Predictions with a Confusion Matrix\n",
    "\n",
    "A confusion matrix provides a detailed breakdown of the model's predictions compared to the true labels. It helps identifyh which classes are being confused with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed Performance with a Classification Report\n",
    "\n",
    "The classification report provides precision, recall, and F1-scores for each class, offering a more nuanced view of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Challenge**: Critically assess the performance of our model based on the accuracy/loss values, confusion matrix, and classification report. Are there any classes that the model is particularly good or bad at predicting? Think about the data and why the model might be performing better or worse for certain classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Transfer Learning with VGG19\n",
    "\n",
    "In this section, we apply a technique called **transfer learning** to improve model performance on our coral classification task.\n",
    "\n",
    "**Transfer learning** is a deep learning technique where we *reuse a model that has already been trained on a large dataset for a different but related task*. Instead of starting from scratch, we \"transfer\" the knowledge learned by the pre-trained model to our new task.\n",
    "\n",
    "This is especially useful when you have a limited dataset, you want to train a model faster, or you want to achieve better accuracy with less computational effort.\n",
    "\n",
    "We use the **VGG19 model**, a classic convolutional neural network developed by researchers at Oxford University. It was trained on **ImageNet** dataset, which contains over 14 million images across 1000 classes.\n",
    "\n",
    "### Step 1: Prepare Data for VGG19\n",
    "\n",
    "#### 1.1 Define Image Preprocessing and Augmentation\n",
    "VGG19 expects input images to be preprocessed in a very specific way because of the way it was trained. We use the `preprocess_input()` function from `tensorflow.keras.applications.vgg19` to preprocess our images. Specifically, this function converts RGB pixel values to the format VGG19 was originally trained on (i.e., channels in BGR order, zero-centered with respect to ImageNet).\n",
    "\n",
    "Let's create new data generators for VGG19 using `ImageDataGenerator` with:\n",
    " - `preprocess_input` for normalization\n",
    " - Augmentation on the training set\n",
    " - No augmentation on the validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Images Using `flow_from_dataframe()'\n",
    "\n",
    "Just like we did for our CNN model, we can use `flow_from_dataframe()` to load images in batches directly from our labeled DataFrames (`train_df`, `val_df`, and `test_df`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define and Train the VGG19 Model\n",
    "#### 2.1 Load VGG19 Base Model and Stack a Custom Classifier\n",
    "\n",
    "We now load the **VGG19 base model**, which has been pre-trained on ImageNet. We exclude the original classification head (`include_top=False`) and freeze all convolutional layers for now.\n",
    "\n",
    "Next, we stack a **custom classifier** on top using Keras’ `Sequential` API:\n",
    "- Flatten the output of VGG19’s last convolutional layer\n",
    "- Add the same fully connected (dense) layers that we had in our original CNN built from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compile the model with the same optimizer and loss function as our previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Define Training Callbacks\n",
    "Next, let's define some **training callbacks**. Callbacks are functions executed during training that allow the training process to change its behavior dynamically.\n",
    "\n",
    "Some common callbacks include:\n",
    "- **EarlyStopping**: This callback stops training when a monitored metric (e.g., validation accuracy) stops improving. It helps prevent overfitting by halting training once the model's performance plateaus.\n",
    "- **ReduceLROnPlateau**: This callback reduces the learning rate when a monitored metric (e.g., validation loss) stops improving. By lowering the learning rate, the model can converge to a better local minimum (preventing it from getting stuck in a suboptimal solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Training History\n",
    "Just like we did for our first CNN model, let's plot the training and validation performance over time. \n",
    "\n",
    "Refer back to Section 1: Step 6 – *Visualizing Training History* for a refresher on how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Challenge**: Compare the performance of our VGG19 model to our previous CNN model. What are some major differences in the training curves?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate the VGG19 Model on the Test Set\n",
    "\n",
    "Just like we did for our first CNN model, let's evaluate the VGG19 model on the test set.\n",
    "\n",
    "#### Evaluate Test Accuracy and Loss\n",
    "First, let's calculate the test accuracy and loss. Can you recall how to do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model correctly classifies the test images about 84% of the time. What an improvement!\n",
    "\n",
    "#### Visualize Predictions with a Confusion Matrix\n",
    "Now, let's visualize the predictions of our VGG19 model on the test set with a confusion matrix.\n",
    "\n",
    "Refer back to Section 1: Step 7 – *Visualize Predictions with a Confusion Matrix* for a refresher on how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the confusion matrix shows a distinct diagonal pattern, where the true and predicted labels are the same more often than not? This indicates that our model is performing well on all classes. Nice!\n",
    "\n",
    "#### Detailed Performance with a Classification Report\n",
    "Finally, let's print out the full classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Challenge**: Compare the performance of our VGG19 model to our previous CNN model. What are some major differences in the classification report? Are there still any problematic classes that the model is struggling with? If so, what do you think is causing this?\n",
    "\n",
    "### Step 4: Visualize Predictions from the Test Set\n",
    "Let's display a few test images along with their predicted labels, true labels, and the model's confidence scores.\n",
    "\n",
    "This helps visually confirm whether predictions make sense – and helps identify patterns in misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts and Wrap-Up\n",
    "\n",
    "Congratulations! You've now built and evaluated two deep learning models for image classification:\n",
    "\n",
    "1. **A CNN from scratch**  \n",
    "   - Gave you hands-on experience building a model layer-by-layer\n",
    "   - Showed the challenges of training with limited data (e.g. overfitting)\n",
    "\n",
    "2. **A Transfer Learning model using VGG19**  \n",
    "   - Leveraged features learned from millions of images (ImageNet)\n",
    "   - Achieved higher accuracy and better generalization with fewer parameters and less training time\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "If you're interested in improving this model further, here are some ideas to help push toward **perfect classification accuracy** on this dataset:\n",
    "\n",
    "- **Fine-tune VGG19**: Unfreeze some of the deeper convolutional layers and retrain the model to better adapt to your specific dataset.\n",
    "- **Explore Other Architectures**: Experiment with different pre-trained models like ResNet or Inception to compare their performance with VGG19.\n",
    "- **Enhance Data Augmentation**: Implement more aggressive data augmentation techniques such as color jitter, brightness shifts, cropping, and noise addition to increase model robustness.\n",
    "- **Improve Image Quality**: Apply image cleaning or filtering techniques to enhance the quality of your dataset.\n",
    "- **Optimize Model Architecture**: Consider adding Batch Normalization, Dropout, or other regularization techniques to improve model generalization.\n",
    "---\n",
    "\n",
    "### Contribute to This Tutorial!\n",
    "\n",
    "We encourage you to share your improvements and insights with the community. If you develop a model that surpasses our current implementation, we'd love to see it!\n",
    "\n",
    "Here's how you can contribute:\n",
    "\n",
    "- **Fork the Repository**: Create your own copy of the repository to work on.\n",
    "- **Enhance and Document**: Add your new model architecture, results, and any notes or observations.\n",
    "- S**ubmit a Pull Request**: Share your improvements by submitting a pull request to contribute to this tutorial.\n",
    "\n",
    "Let's see what you can build!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
